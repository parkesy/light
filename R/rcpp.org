In this document I'm going to investigate the importance of integrating R and C++.

* Whats the problem?
I've heard a lot of people say that R is really slow and I'd like to take a some time to quickly investigate that observation.

Some basic assumptions:
- People are not suggesting replacing R with C++. That would be silly! since R's interpreter is written in C.
- R's interpreter is written well and has as minimal overhead as possible.
- The conjecture is that you can improve performance by specialising some functions.

** Timing
In order to time functions for speed comparison I've had to write my own timer function. R has a built in feature called *system.time(x())* which will time how long x takes to execute. The problem with it is that its resolution is seconds. Below is my timer:
#+begin_src C++ :flags std=c++11 :tangle yes :tangle src/timer.cpp :main no
#include <chrono>
#include <Rcpp.h>
#include <R.h>

RcppExport SEXP Time(SEXP function)
{
    Rcpp::Function f = function;
    
    std::chrono::high_resolution_clock::time_point before = 
	    std::chrono::high_resolution_clock::now();
    
    f();
    
    std::chrono::high_resolution_clock::time_point after = 
	    std::chrono::high_resolution_clock::now();
    
    std::chrono::duration<double> timeSpan = 
        std::chrono::duration_cast<std::chrono::duration<double>>(after - before);

    return Rcpp::wrap(timeSpan.count());
} 
#+end_src

Compiling and linking step
#+begin_src sh :results output
g++ -c src/timer.cpp -o src/timer.o -std=c++11 -I /home/parkesy/R/i686-pc-linux-gnu-library/3.0/Rcpp/include/ -I /usr/share/R/include/ && g++ -shared src/timer.o -o src/timer.so -O3 && echo "Done" 
#+end_src

#+RESULTS:
: Done

** Plotting the data
I'm going to use the below function to plot the time complexity of the various functions.
#+begin_src R :session *R* :results output :tangle yes :tangle src/plotter.R
library(Rcpp)
dyn.load('src/timer.so')

#accepts a function with no arguments. 
Timer <- function(f)
{
    .Call("Time", f)
}

empty.function <- function() {}
dyncall.overhead.data <- unlist(Map(function(x) Timer(empty.function), seq(1,30000)))
dyncall.overhead.nanoseconds <- summary(dyncall.overhead.data * 1000000000)[3] # median
#+end_src


#+begin_src R
#wrapper.sequence <- function(itr)
#{
#	gc(reset=TRUE)
#	data <- seq(1,itr)
#	.Call("Time", function() f(data, itr))
#}
#
#wrapper.no.param <- function(itr)
#{
#	gc(reset=TRUE)
#	.Call("Time", f)
#}
#
#PlotComplexity <- function(name, sequence, f, timingWrapper)
#{
#    t <- unlist(Map(timingWrapper, sequence))
# 
#    df <- data.frame(t, N=sequence);
#    layout(matrix(c(1,2), ncol=2))
#    plot((df$t * 1000000) ~ df$N, main=name, ylab="Time in microseconds", xlab="N")
#    grid()
#    reg <- lm((df$t * 1000000) ~ df$N)
#    abline(reg, col="red")
#    hist(df$t * 1000000, col="blue", main="Histogram of time", ylab="Time in microseconds", breaks=30)
#    grid()
#}
#+end_src

#+RESULTS:

** Complexity O(1)
The measurement tool that I have available in R doesn't provide the resolution needed to measure the time taken to get a value at an index. 

#+begin_src R :results graphics :file img/O1.jpg :session *R* :width 1000
PlotComplexity("O(1)", seq(2, 1000) * 100, function(a, x) { })
#+end_src

#+RESULTS:
[[file:img/O1.jpg]]


So I'm going to do a bit of hand waving here and say that C++ is not going to be a viable replacement for indexing into a vector. I make this assertion because compiling a minimal c++ plugin for R can take (on this computer) about 3 seconds, in order to gain an improvement we'd have to make sure that C++ indexing logic is faster than R's indexing logic. Even if its twice as fast we'd still need to do over many thousands of indexing operations in order to break even. Its very unlikely that it would even be 10% faster. </speculation>

** Complexity O(log n)
Something that I think R is missing is a function to find the index of a element in a sorted list. How do we currently do using R built ins? like this
#+begin_src R :session *R* :results graphics :file img/baseRLogN.jpg :tangle yes :tangle src/baseRLogN.R 
PlotComplexityAt2ToThe("Base functionality 0(log N)", function(a, x)
{
    which(a == trunc(x/2))
})
#+end_src

#+RESULTS:
[[file:img/baseRLogN.jpg]]
This looks more O(N) not log(N).

How can c++ help? we can specialise for case the algorithm is called.... binary search.
#+begin_src R :results output :results graphics :file img/rcppLongN.jpg :session *R*
library('inline')
library('Rcpp')
src <- '
    Rcpp::NumericVector x(xs);

    if (x.size() <= 0)
        return Rcpp::wrap(0);

    int minIndex = 0;
    int maxIndex = x.size();
    const int target = Rcpp::as<int>(t);
    while (maxIndex >= minIndex)
    {
        const int mid = (minIndex + maxIndex) / 2;
        if (x[mid] == target)
            return Rcpp::wrap(mid + 1);
        else if (x[mid] < target)
            minIndex = mid + 1;
        else 
            maxIndex = mid - 1;
    } 
    return Rcpp::wrap(0);'

binarySearch <- cxxfunction(signature(xs="numeric", t="numberic"), body=src, plugin="Rcpp")

PlotComplexityAt2ToThe("Rcpp O(log N)", function(a, x)
{
    binarySearch(a, trunc(x/2) + 1)
})
#+end_src

#+RESULTS:
[[file:img/rcppLongN.jpg]]
The run time is much faster... but the compile time is still a killer.

With some more research I saw that you _can_ get R to perform a binary search with the following steps. *installed data.table, if you don't already have it*
#+begin_src R :results output :results graphics :file img/datatableLogN.jpg :session *R*
library('data.table')

binarySearch <- function(data, target)
{
#magic incantation to the R gods
    dt <- data.table(data, val = data)
    setattr(dt, "sorted", "data")
    dt[J(target), roll="nearest"]
}

PlotComplexityAt2ToThe("data.table O(log N)", function(a, x)
{
    binarySearch(a, trunc(x/2) + 1)
})
#+end_src

#+RESULTS:
[[file:img/datatableLogN.jpg]]

Now we have very similar results to the C++ solution we built, but without the compilation step*.

*When installing data.table you'll notice that its doing a lot of C++ compilation. I'd hazard a guess and say that author of data.table spotted some of the issues that we observed and has tried to fix them using C/C++.

The thing that is note worthy here is that we needed to install data.table, which compiled some C/C++ in order to gain extra functionality. This observation leads to our point, the use of C/C++ is good for speeding up operations; i.e. if you want to specialise for speed the C/C++ is your best option. The thing that we are going to keep discovering is that compilation is a concern.

What about implementing Binary search in R. That is a great idea, the implicit question here is why does R have all these vector operations and are they faster then the non vector implementation.
#+begin_src R :results graphics :tangle yes :tangle src/baseBinarySearch.R :session *R* :file img/rImplLogN.jpg 
binarySearch <- function(data, target)
{
    if (length(data) <= 0)
    {
        return(NaN)
    }

    minIndex = 1
    maxIndex = length(data)
    while (maxIndex >= minIndex)
    {
        mid = trunc((minIndex + maxIndex) / 2)
        
        if (data[mid] == target)
        {
            return(mid)
        }
        else if (data[mid] < target)
        {
            minIndex = mid + 1
        }
        else
        { 
            maxIndex = mid - 1
        }
    }
    return(NaN)
}

PlotComplexityAt2ToThe("R Impl O(log N)", function(a, x)
{
    binarySearch(a, trunc(x/2) +1)
})
#+end_src 

#+RESULTS:
[[file:img/rImplLogN.jpg]]

Interesting result! It is faster. But why? because our other tests are measuring other things. In the data.table we are measuring the time taken to allocated new memory. This is quite obvious when you think about the order of complexity O(log N); i.e. log 30000000 = 17.2 to do 17 iterations is very quick.
*** Quick test
Deeper look into the cost using data.table
#+begin_src R :results output 
library('data.table')
a <- seq(1,30000000)

binarySearch <- function(data, target)
{
    #magic incantation to the R gods
    dt <- data.table(data, val = data)
    setattr(dt, "sorted", "data")
    dt[J(target), roll="nearest"]
}

system.time(binarySearch(a, 12345678))


cat("\nAllocation time\n")
system.time(dt <- data.table(a, val = a))
cat("\nSetting Attr time\n")
system.time(setattr(dt, "sorted", "a"))
cat("\nActual algo time\n")
system.time(dt[J(12345678), roll="nearest"])

cat("\nfunction call time\n")
test <- function(data, target)
{
}
system.time(test(a, 12345678))

cat("\nfunction call with assignment and return value time\n")
test <- function(data, target)
{
    dt <- data.table(a, val = a)
    return(dt)
}
system.time(test(a, 12345678))

cat("\nfunction call with assignment and return value time. v2\n")
test <- function(data, target)
{
    return(data.table(a, val = a))
}
system.time(test(a, 12345678))
#+end_src

#+RESULTS:
#+begin_example
   user  system elapsed 
  0.052   0.028   0.080 

Allocation time
   user  system elapsed 
  0.020   0.024   0.043 

Setting Attr time
   user  system elapsed 
      0       0       0 

Actual algo time
   user  system elapsed 
  0.000   0.000   0.001 

function call time
   user  system elapsed 
      0       0       0 

function call with assignment and return value time
   user  system elapsed 
  0.048   0.024   0.073 

function call with assignment and return value time. v2
   user  system elapsed 
  0.028   0.016   0.042 
#+end_example
** Complexity O(n)
Lets take a simple operation. Here we are going to try and find the minimum element in an array.
Complexity O(n).
#+begin_src R :results graphics :session *R* :file img/baseRN.jpg 
PlotComplexityAt2ToThe("basic O(N)", function(a, x)
{
    system.time(min(a))
})
#+end_src

#+RESULTS:
[[file:img/baseRN.jpg]]

Now lets do the same in c++. 
#+begin_src R :results graphics :session *R* :file img/rcppN.jpg
library('inline')
library('Rcpp')
src <- '
    Rcpp::NumericVector x(xs);
    Rcpp::NumericVector::iterator it =       // iterator type
        std::min_element(x.begin(), x.end());  // STL algo
    return Rcpp::wrap(*it);'
minfun <- cxxfunction(signature(xs="numeric"), body=src, plugin="Rcpp")

PlotComplexityAt2ToThe("Rcpp O(N)", function(a, x)
{
    minfun(a)
})
#+end_src  

#+RESULTS:
[[file:img/rcppN.jpg]]

We had some gains in performance, approximately 16% speed improvement. That is am improvement if you ignore the compilation step.

Lets not ignore that. How much is it really costing us?
#+begin_src R :results graphics :session *R* :file img/rcppOn.jpg
library('inline')
library('Rcpp')

src <- '
Rcpp::NumericVector x(xs);
Rcpp::NumericVector::iterator it =       // iterator type
	std::min_element(x.begin(), x.end());  // STL algo
return Rcpp::wrap(*it);'
minfun <- cxxfunction(signature(xs="numeric"), body=src, plugin="Rcpp")

PlotComplexityAt2ToThe("Rcpp O(N)", function(a,x)
{
    minfun(a)
})
#+end_src

#+RESULTS:
[[file:img/rcppOn.jpg]]

I bundled the compilation and the execution together so that we could get an overall timing. In the real world you'd be mad to do this. You would simply compile and assign the C++ function to a R symbol in your global environment once and once only. *Or you'd learn how to make a R package*

*** Summary
We pay a bulk one off time cost for compilation then we gain a small increase in performance when running the function. 

What have we really done though? we have created a Min function that takes a list of R::Numerics but treats them all as integers for simplicity sake. The function should have been called MinInteger. 

*What if this simple approach causing performance gain?*
On the one hand you have the frustration of having to build specialisations for MinDouble, MinCharacter and the inconvenience of needing to call specialised functions with correct data. This might be a fine solution for some or it might not. The only way to tell is to measure how long your report takes to build and contrast that with:
- the added complexity of building/maintaining native c++ code.
- the added time to compile
- the added tightly coupled/simplified c++ solution that often will be generated. 
- the gains that you potentially will get with a native c++ solution
 
** Complexity O(n log n)
I'll take an educated guess and say that the differences we are going to find from here on out are between algorithmic implementation and type checking. Its fare to say that when we specialise function we are only going to cater for one data type. Our implementation will always have an advantage R. The other thing we can take advantage of is known data ordering. Take the O(log n) test. We knew the order of the data and we could take advantage of that fact and implement a binarySearch. Using base R functions we could not do that. 

...PENDING

* Is R really slow?
The message, from the above tests, is that if you want to run c++ and R together the functions that you are specialising with c++ need to be compiled once then set into a function and that function _needs_ to be called a lot in order to compensate for the time lost to compilation.


Formula to figure out if you should be optimising with c++. Assuming all of the above tests scale linearly... which I'm confident they wouldn't. But lets just do this for fun. 
#+begin_src R :results graphics :file img/rcpp-rvcpp.jpg
ShouldIOptimise <- function (RFunctionExecTime, numberOfTimesUsed)
{
    # We are assumming that we will always get avg perfromance gain of 17% 
    cppRunTime = RFunctionExecTime - RFunctionExecTime * 0.17;
    
    # we are assuming that compilation will take 120% longer than running the function
    cppCompTime = cppRunTime * 1.2

    return (data.frame(
        optimise=((RFunctionExecTime * numberOfTimesUsed) > (cppCompTime + cppRunTime * numberOfTimesUsed)), 
        Rtime=(RFunctionExecTime * numberOfTimesUsed),
        cppTotal=(cppCompTime + cppRunTime * numberOfTimesUsed)))
}

result <- Map(function(x) ShouldIOptimise(1.724, x), seq(1,10))

df.results <- data.frame(
    opt=unlist(Map(function(x) x[,1], result)),
    rtime=unlist(Map(function(x) x[,2], result)),
    cpptime=unlist(Map(function(x) x[,3], result)))

plot(df.results$rtime, type="l", main="Mock R vs Rcpp time cost*", col="blue", ylab="seconds", xlab="function calls")
lines(df.results$cpptime, col="red")
legend("topleft", legend = c("R time", "C++ time"), col=c("blue", "red"), pch="l")
grid()
#+end_src

#+RESULTS:
[[file:img/rcpp-rvcpp.jpg]]


*Remember this is not an accurate model to base your decisions off. You'll need to quickly test your c++ functions vs the equivalent R function to best make an informed decision. Its very likely that as the complexity of the function increases R execution time will reduce and gains from c++ implementation will start to really out perform. 
