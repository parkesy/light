This document started out with the intent of showing that specialising R functions using C++ lead to better performance and faster report generation for data sets that are large but not "Big data" large.

* Whats the problem?
I've heard a lot of people say that R is really slow and I'd like to take a some time to quickly investigate that observation.

Some basic assumptions:
- People are not suggesting replacing R with C++. That would be silly! since R's interpreter is written in C.
- R's interpreter is written well and has as minimal overhead as possible.
- The conjecture is that you can improve performance by specialising some functions.

** Timing
In order to time functions for speed comparison I've had to write my own timer function. R has a built in feature called *system.time(x())* which will time how long a function takes to execute. The problem with it is that its resolution is seconds with no decimal places. Below is my timer code:
#+begin_src C++ :flags std=c++11 :tangle yes :tangle src/timer.cpp :main no
#include <chrono>
#include <Rcpp.h>
#include <R.h>

RcppExport SEXP Time(SEXP function)
{
    Rcpp::Function f = function;
    
    std::chrono::high_resolution_clock::time_point before = 
	    std::chrono::high_resolution_clock::now();
    
    f();
    
    std::chrono::high_resolution_clock::time_point after = 
	    std::chrono::high_resolution_clock::now();
    
    std::chrono::duration<double> timeSpan = 
        std::chrono::duration_cast<std::chrono::duration<double>>(after - before);

    return Rcpp::wrap(timeSpan.count());
} 
#+end_src

#+RESULTS:

Compiling and linking step
#+begin_src sh :results output
g++ -c src/timer.cpp -o src/timer.o -std=c++11 -I /home/parkesy/R/i686-pc-linux-gnu-library/3.0/Rcpp/include/ -I /usr/share/R/include/ && g++ -shared src/timer.o -o src/timer.so -Ofast && echo "Done" 
#+end_src

** Plotting the data
I'm going to use the below function to plot the time complexity of the various functions.

We need to get a baseline for the any overhead in call timing a function. We'll time how long it takes to call an empty function thirty thousand times. This will show us a baseline.
#+begin_src R :session *R* :results graphics :file img/baseline.jpg
library(Rcpp)
dyn.load('src/timer.so')

#accepts a function with no arguments. 
Timer <- function(f)
{
    .Call("Time", f)
}

empty.function <- function() {}
dyncall.overhead.data <- unlist(Map(function(x) Timer(empty.function), seq(1,30000)))
(dyncall.overhead.nanoseconds <- summary(dyncall.overhead.data * 1000000000)[3]) # median

plot(dyncall.overhead.data * 1000000, ylab="microseconds")
#+end_src

#+RESULTS:
[[file:img/baseline.jpg]]

#+begin_src R :session *R* :results output
summary(dyncall.overhead.data * 1000000)
#+end_src

#+RESULTS:
:     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
:    8.897    9.446    9.720   11.150   10.280 1801.000

Extending our timer We can use Timer to time functions with no parameters. 
#+begin_src R :session *R* 
TimeWithSequenceAndX <- function(func, sequence, x)
{
    Timer(function() func(sequence, x))
}

GenerateComplexity <- function(f, Ns)
{
    times <- unlist(
        Map(function(indx)
            TimeWithSequenceAndX(f, seq(1, indx), indx), 
        Ns)) # N complexities
    return(data.frame(times=times * 1000000, N=Ns))  
}

PlotComplexity <- function(name, ComplexityDf)
{
    layout(matrix(c(1,2), ncol=2))
    plot(df$times ~ df$N, main=name, ylab="Time in microseconds", xlab="N")
    grid()
    reg <- lm(df$t ~ df$N)
    abline(reg, col="red")
    hist(df$t * 1000000, col="blue", main="Histogram of time", ylab="Time in microseconds", breaks=30)
    grid()
}
#+end_src

#+RESULTS:

** Complexity O(1)
The measurement tool that I have available in R doesn't provide the resolution needed to measure the time taken to get a value at an index. 

#+begin_src R :results graphics :file img/O1.jpg :session *R* :width 1000
df <- GenerateComplexity(function(a,x){}, seq(2,1000) * 100)
PlotComplexity("O(1)", df)
#+end_src

#+RESULTS:
[[file:img/O1.jpg]]

#+begin_src R :session *R*
summary(df)
#+end_src

#+RESULTS:
| Min.   :   9.893 | Min.   :   200 |
| 1st Qu.:  11.309 | 1st Qu.: 25150 |
| Median :  11.718 | Median : 50100 |
| Mean   :  13.193 | Mean   : 50100 |
| 3rd Qu.:  12.134 | 3rd Qu.: 75050 |
| Max.   :1208.525 | Max.   :100000 |

** Complexity O(log n)
Something that I think R is missing is a function to find the index of a element in a sorted list. How do we currently do using R built ins? like this
#+begin_src R :session *R* :results graphics :file img/baseRLogN.jpg :tangle yes :tangle src/baseRLogN.R :width 1000 
df <- GenerateComplexity(function(a,x) 
    which(a == (trunc(x/2) + 1)),
    seq(2, 100003, by=1000))

PlotComplexity("Base functionality O(log N)", df)
#+end_src

#+RESULTS:
[[file:img/baseRLogN.jpg]]

#+begin_src R :session *R* :results table
summary(df)
#+end_src 

#+RESULTS:
| Min.   :  92.72 | Min.   :     2 |
| 1st Qu.: 501.65 | 1st Qu.: 25002 |
| Median : 778.39 | Median : 50002 |
| Mean   : 858.07 | Mean   : 50002 |
| 3rd Qu.:1175.35 | 3rd Qu.: 75002 |
| Max.   :2664.99 | Max.   :100002 |

#+begin_src R :results output :results graphics :file img/rcppLongN.jpg :session *R* :width 1000
library('inline')
library('Rcpp')
src <- '
    Rcpp::NumericVector x(xs);

    if (x.size() <= 0)
        return Rcpp::wrap(0);

    int minIndex = 0;
    int maxIndex = x.size();
    const int target = Rcpp::as<int>(t);
    while (maxIndex >= minIndex)
    {
        const int mid = (minIndex + maxIndex) / 2;
        if (x[mid] == target)
            return Rcpp::wrap(mid + 1);
        else if (x[mid] < target)
            minIndex = mid + 1;
        else 
            maxIndex = mid - 1;
    } 
    return Rcpp::wrap(0);'

binarySearch <- cxxfunction(signature(xs="numeric", t="numberic"), body=src, plugin="Rcpp")

df <- GenerateComplexity(function(s,x) binarySearch(s, trunc(x/2)+1) , seq(2, 100002, by=1000))
PlotComplexity("Rcpp O(log N)", df)
#+end_src

#+RESULTS:
[[file:img/rcppLongN.jpg]]

#+begin_src R :session *R*
summary(df)
#+end_src

#+RESULTS:
| Min.   : 124.3 | Min.   :     2 |
| 1st Qu.: 198.7 | 1st Qu.: 25002 |
| Median : 230.4 | Median : 50002 |
| Mean   : 279.0 | Mean   : 50002 |
| 3rd Qu.: 274.2 | 3rd Qu.: 75002 |
| Max.   :3637.1 | Max.   :100002 |

#+begin_src R :results output :results graphics :file img/datatableLogN.jpg :session *R* :width 1000
library('data.table')

binarySearch <- function(data, target)
{
#magic incantation to the R gods
    dt <- data.table(data, val = data)
    setattr(dt, "sorted", "data")
    dt[J(trunc(target/2)+1), roll="nearest"]
}

df <- GenerateComplexity(binarySearch, seq(2, 100002, by=1000))
PlotComplexity("Data.table O(log N)", df)
#+end_src

#+RESULTS:
[[file:img/datatableLogN.jpg]]

#+begin_src R :session *R*
summary(df)
#+end_src

#+RESULTS:
| Min.   : 927.3 | Min.   :     2 |
| 1st Qu.: 976.3 | 1st Qu.: 25002 |
| Median :1002.8 | Median : 50002 |
| Mean   :1070.5 | Mean   : 50002 |
| 3rd Qu.:1042.5 | 3rd Qu.: 75002 |
| Max.   :2643.0 | Max.   :100002 |

#+begin_src R :results graphics :tangle yes :tangle src/baseBinarySearch.R :session *R* :file img/rImplLogN.jpg :width 1000
binarySearch <- function(data, target)
{
    if (length(data) <= 0)
    {
        return(NaN)
    }

    minIndex = 1
    maxIndex = length(data)
    while (maxIndex >= minIndex)
    {
        mid = trunc((minIndex + maxIndex) / 2)
        
        if (data[mid] == target)
        {
            return(mid)
        }
        else if (data[mid] < target)
        {
            minIndex = mid + 1
        }
        else
        { 
            maxIndex = mid - 1
        }
    }
    return(NaN)
}

df <- GenerateComplexity(function(a, x) binarySearch(a, trunc(x/2) +1), seq(2, 100002, by=1000))
PlotComplexity("R Impl O(log N)", df)
#+end_src 

#+RESULTS:
[[file:img/rImplLogN.jpg]]

#+begin_src R :session *R*
summary(df)
#+end_src 

#+RESULTS:
| Min.   : 70.83 | Min.   :     2 |
| 1st Qu.: 96.67 | 1st Qu.: 25002 |
| Median :113.21 | Median : 50002 |
| Mean   :146.09 | Mean   : 50002 |
| 3rd Qu.:143.51 | 3rd Qu.: 75002 |
| Max.   :438.01 | Max.   :100002 |

** Complexity O(n)
Lets take a simple operation. Here we are going to try and find the minimum element in an array.
Complexity O(n).
#+begin_src R :results graphics :session *R* :file img/baseRN.jpg :width 1000
df <- GenerateComplexity(function(a,x) min(a), seq(2, 100002, by=1000))
PlotComplexity("Basic O(N)", df)
#+end_src

#+RESULTS:
[[file:img/baseRN.jpg]]

#+begin_src R :session *R* :results output
summary(df)
#+end_src 

#+RESULTS:
:      times               N         
:  Min.   :  30.30   Min.   :     2  
:  1st Qu.:  63.41   1st Qu.: 25002  
:  Median : 109.71   Median : 50002  
:  Mean   : 154.60   Mean   : 50002  
:  3rd Qu.: 182.47   3rd Qu.: 75002  
:  Max.   :2632.31   Max.   :100002

Now lets do the same in c++. 
#+begin_src R :results graphics :session *R* :file img/rcppN.jpg :width 1000
library('inline')
library('Rcpp')
src <- '
    Rcpp::NumericVector x(xs);
    Rcpp::NumericVector::iterator it =       // iterator type
        std::min_element(x.begin(), x.end());  // STL algo
    return Rcpp::wrap(*it);'
minfun <- cxxfunction(signature(xs="numeric"), body=src, plugin="Rcpp")

df <- GenerateComplexity(function(a,x) minfun(a), seq(2, 100002, by=1000))
PlotComplexity("Rcpp O(N)", df) 
#+end_src  

#+RESULTS:
[[file:img/rcppN.jpg]]

#+begin_src R :session *R* :results output
summary(df)
#+end_src 

#+RESULTS:
:      times               N         
:  Min.   :  45.75   Min.   :     2  
:  1st Qu.: 253.67   1st Qu.: 25002  
:  Median : 443.64   Median : 50002  
:  Mean   : 446.78   Mean   : 50002  
:  3rd Qu.: 587.46   3rd Qu.: 75002  
:  Max.   :1863.57   Max.   :100002


*** Summary
We pay a bulk one off time cost for compilation then we gain a small increase in performance when running the function. 

What have we really done though? we have created a Min function that takes a list of R::Numerics but treats them all as integers for simplicity sake. The function should have been called MinInteger. 

*What if this simple approach causing performance gain?*
On the one hand you have the frustration of having to build specialisations for MinDouble, MinCharacter and the inconvenience of needing to call specialised functions with correct data. This might be a fine solution for some or it might not. The only way to tell is to measure how long your report takes to build and contrast that with:
- the added complexity of building/maintaining native c++ code.
- the added time to compile
- the added tightly coupled/simplified c++ solution that often will be generated. 
- the gains that you potentially will get with a native c++ solution
 
** Complexity O(n log n)
I'll take an educated guess and say that the differences we are going to find from here on out are between algorithmic implementation and type checking. Its fare to say that when we specialise function we are only going to cater for one data type. Our implementation will always have an advantage R. The other thing we can take advantage of is known data ordering. Take the O(log n) test. We knew the order of the data and we could take advantage of that fact and implement a binarySearch. Using base R functions we could not do that. 

...PENDING

* Is R really slow?
The message, from the above tests, is that if you want to run c++ and R together the functions that you are specialising with c++ need to be compiled once then set into a function and that function _needs_ to be called a lot in order to compensate for the time lost to compilation.


Formula to figure out if you should be optimising with c++. Assuming all of the above tests scale linearly... which I'm confident they wouldn't. But lets just do this for fun. 
#+begin_src R :results graphics :file img/rcpp-rvcpp.jpg
ShouldIOptimise <- function (RFunctionExecTime, numberOfTimesUsed)
{
    # We are assumming that we will always get avg perfromance gain of 17% 
    cppRunTime = RFunctionExecTime - RFunctionExecTime * 0.17;
    
    # we are assuming that compilation will take 120% longer than running the function
    cppCompTime = cppRunTime * 1.2

    return (data.frame(
        optimise=((RFunctionExecTime * numberOfTimesUsed) > (cppCompTime + cppRunTime * numberOfTimesUsed)), 
        Rtime=(RFunctionExecTime * numberOfTimesUsed),
        cppTotal=(cppCompTime + cppRunTime * numberOfTimesUsed)))
}

result <- Map(function(x) ShouldIOptimise(1.724, x), seq(1,10))

df.results <- data.frame(
    opt=unlist(Map(function(x) x[,1], result)),
    rtime=unlist(Map(function(x) x[,2], result)),
    cpptime=unlist(Map(function(x) x[,3], result)))

plot(df.results$rtime, type="l", main="Mock R vs Rcpp time cost*", col="blue", ylab="seconds", xlab="function calls")
lines(df.results$cpptime, col="red")
legend("topleft", legend = c("R time", "C++ time"), col=c("blue", "red"), pch="l")
grid()
#+end_src

#+RESULTS:
[[file:img/rcpp-rvcpp.jpg]]


*Remember this is not an accurate model to base your decisions off. You'll need to quickly test your c++ functions vs the equivalent R function to best make an informed decision. Its very likely that as the complexity of the function increases R execution time will reduce and gains from c++ implementation will start to really out perform. 
